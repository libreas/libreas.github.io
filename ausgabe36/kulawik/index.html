
<!DOCTYPE HTML>

<html lang="de">

<head>
	<meta charset="utf-8">
	<title>Überlegungen zu einer nachhaltigen Forschungsdateninfrastruktur - LIBREAS. Library Ideas</title>
	<meta name="author" content="LIBREAS. Library Ideas">


<!-- RDFa DC -->

 <!-- RDFa Metadata (in DublinCore) -->
    
    <meta property="dc:title" content="Überlegungen zu einer nachhaltigen Forschungsdateninfrastruktur" />
    
    
    
    <meta property="dc:creator" content="Bernd Kulawik" />
    
    
    
    <meta property="dc:date" content="2019" />
    
    <meta property="dc:format" content="text/html" />
    <meta property="dc:language" content="de" />
    <meta property="dc:identifier" content="https://libreas.eu/ausgabe36/kulawik/index.html" />
    
    <meta property="dc:rights" prefix="cc: http://creativecommons.org/ns#" rel="cc:license" href="https://creativecommons.org/licenses/by/4.0/" content="CC-BY 4.0" />
	
    <meta property="dc:source" content="LIBREAS. Library Ideas" />
    <meta property="dc:subject" content="Library and Information Science" />
    <meta property="dc:type" content="website" />
    <meta property="dc:description" content="In diesem Essay wird eine radikale Lösung des Problems wirklich langfristiger Datensicherung und -verfügbarkeit vorgeschlagen. Sie beruht auf der zuvor erläuterten Einschätzung, dass die Vielfalt und Komplexität heutiger IT-Systeme weder mit vertretbarem Aufwand für die Zukunft konserviert werden kann, noch in der Zukunft rekonstruierbar sein wird. Ein Ausweg könnte die Schaffung einer (internationalen) Institution mit &lt;q&gt;Ewigkeitsgarantie&lt;/q&gt; sein, welche die Entwicklung und Betreuung einer freien Hard- und Softwareplattform übernimmt, mit der insbesondere Forschungsprojekte und -einrichtungen, aber auch Firmen oder Privatpersonen arbeiten können, die ihre Daten abschließend der Institution zur dauerhaften Speicherung übergeben wollen. Bei der Entwicklung dieser Plattform könnten vorausweisende Lösungen aus der bisherigen IT-Enwicklung übernommen; bekannte, aber aus Kompatibilitätsgründen nicht behobene Fehler und Irrtümer jedoch vermieden werden. Außerdem könnte das System zugleich als eine wirklich offene Plattform für Repositorien zur Veröffentlichung von Daten und Ergebnissen im Open Access bilden und undurchsichtige Verfahren beispielsweise im Peer Review und der Messung von Forschungsrelevanz ablösen. Da Bibliotheken, Archive und Museen diejenigen Institutionen sind, die über umfangreichste historisch gewachsene Erfahrungen in der Speicherung und Erschließung von Daten im weitesten Sinne haben, sollten sie bei Konzeption und Betrieb dieser Institution eine führende Rolle spielen."/>
    <meta property="dc:source" resource="urn:ISSN:1860-7950" /> 

<!-- Google Scholar -->

	<meta name="citation_journal_title" content="LIBREAS. Library Ideas" />
	<meta name="citation_issue" content="36" />
	<meta name="citation_issn" content="1860-7950" />
	
    <meta name="citation_publication_date" content="2019" />
    


  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="/atom.xml" rel="alternate" title="LIBREAS. Library Ideas" type="application/atom+xml">
	
	<link rel="canonical" href="https://libreas.eu/ausgabe36/kulawik/">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">


    <!--   
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="favicon.png"> -->
  
	
</head>




  <body class="theme-base-wes">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <a href="/">LIBREAS. Library Ideas</a>
  </div>

  <nav class="sidebar-nav">

    	<a class="sidebar-nav-item" href="/ausgabe37/">Aktuelle Ausgabe</a>
	<a class="sidebar-nav-item" href="/archiv.htm">Archiv</a>
	<a class="sidebar-nav-item" href="http://edoc.hu-berlin.de/browsing/libreas/">edoc pdf-Archiv</a>
	<a class="sidebar-nav-item" href="/autorinnen">AutorInnen</a>
	<a class="sidebar-nav-item" href="http://libreas.wordpress.com/category/libreas-call-for-papers/">Call for Papers</a>
	<a class="sidebar-nav-item" href="/authorguides">Hinweise</a>
	<a class="sidebar-nav-item" href="http://libreas.wordpress.com/">Blog</a>
	<a class="sidebar-nav-item" href="http://libreas.tumblr.com/">Tumblr</a>
	<a class="sidebar-nav-item" href="https://web.archive.org/web/20171112100417/http://www.ib.hu-berlin.de/~libreas/libreas_neu/podcasts/">Podcasts</a>
	<a class="sidebar-nav-item" href="http://libreas-verein.eu">Verein</a>
	<a class="sidebar-nav-item" href="/about">Impressum</a>

  </nav>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!-- <div class="wrap">
       <div class="masthead">
        <div class="container">
          <h1 class="masthead-title">
            <a href="/" title="Home">LIBREAS. Library Ideas</a>
            <small><br></small>
          </h1>
        </div>
      </div> -->


      <div class="container content">
      <!-- GitHub et al icon -->
      <div align="right" class="icons">
        <a href="mailto:redaktion@libreas.eu"><i class="fa fa-envelope"></i></a>
        <a href="https://github.com/libreas"><i class="fa fa-github-square"></i></a>
        <a href="https://www.facebook.com/libreas"><i class="fa fa-facebook"></i></a>
        <a href="https://twitter.com/libreas"><i class="fa fa-twitter"></i></a>
        <a href="https://github.com/libreas.atom"><i class="fa fa-rss"></i></a>
</div>
        
<article class="page" itemscope itemtype="http://schema.org/ScholarlyArticle">

<div align="right">

<strong> > > > <a href="/ausgabe36/inhalt/">LIBREAS. Library Ideas # 36</a></strong> <br><br>
</div>




    
    <div align="right">
    <p><small><a itemprop="sameAs" href="https://via.hypothes.is/https://libreas.eu/ausgabe36/kulawik/">Annotate via hypothes.is</a></small></p>
        <small><a itemprop="sameAs" href="http://nbn-resolving.de/urn:nbn:de:kobv:11-110-18452/22144-3">Download PDF @ edoc HU Berlin</a>
            <br>
            urn:nbn:de:kobv:11-110-18452/22144-3 </small>
        </div>
        
    
    
        

        
        <h1 itemprop="name" class="title">Überlegungen zu einer nachhaltigen Forschungsdateninfrastruktur</h1>
        
        

        <div class="titelei">
        
        <i itemprop="author">Bernd Kulawik</i>
        
    </div>

<!-- 

        
        <div class="titelei">
            
              
              
            <i itemprop="author">Bernd Kulawik</i> 
            
            
            
        </div>
 -->



        

        <p><small itemprop="description">In diesem Essay wird eine radikale Lösung des Problems wirklich langfristiger Datensicherung und -verfügbarkeit vorgeschlagen. Sie beruht auf der zuvor erläuterten Einschätzung, dass die Vielfalt und Komplexität heutiger IT-Systeme weder mit vertretbarem Aufwand für die Zukunft konserviert werden kann, noch in der Zukunft rekonstruierbar sein wird. Ein Ausweg könnte die Schaffung einer (internationalen) Institution mit <q>Ewigkeitsgarantie</q> sein, welche die Entwicklung und Betreuung einer freien Hard- und Softwareplattform übernimmt, mit der insbesondere Forschungsprojekte und -einrichtungen, aber auch Firmen oder Privatpersonen arbeiten können, die ihre Daten abschließend der Institution zur dauerhaften Speicherung übergeben wollen. Bei der Entwicklung dieser Plattform könnten vorausweisende Lösungen aus der bisherigen IT-Enwicklung übernommen; bekannte, aber aus Kompatibilitätsgründen nicht behobene Fehler und Irrtümer jedoch vermieden werden. Außerdem könnte das System zugleich als eine wirklich offene Plattform für Repositorien zur Veröffentlichung von Daten und Ergebnissen im Open Access bilden und undurchsichtige Verfahren beispielsweise im Peer Review und der Messung von Forschungsrelevanz ablösen. Da Bibliotheken, Archive und Museen diejenigen Institutionen sind, die über umfangreichste historisch gewachsene Erfahrungen in der Speicherung und Erschließung von Daten im weitesten Sinne haben, sollten sie bei Konzeption und Betrieb dieser Institution eine führende Rolle spielen.</small></p>

        

        



        
        <p>
          <small>
            <br>
            Zitiervorschlag
            <br>
            
            
            Bernd Kulawik,
            
            
             
            "Überlegungen zu einer nachhaltigen Forschungsdateninfrastruktur".
            
            <i itemprop="provider">LIBREAS. Library Ideas</i>, 36 (<span itemprop="datePublished">2019</span>). <a itemprop="url" href="https://libreas.eu/ausgabe36/kulawik/">https://libreas.eu/ausgabe36/kulawik/</a>
        </small>
        <hr>
    </p>
   
    





    <div class="entry-content" itemprop="articleBody"><nav id="TOC" role="doc-toc">
<ul>
<li><a href="#einleitung-zustandsbeschreibung-und-abzuleitende-forderungen">Einleitung: Zustandsbeschreibung und abzuleitende Forderungen</a></li>
<li><a href="#lösungsvorschlag-ein-radikaler-neubeginn">Lösungsvorschlag: ein radikaler Neubeginn</a></li>
<li><a href="#literatur">Literatur</a></li>
</ul>
</nav>
<h3 id="einleitung-zustandsbeschreibung-und-abzuleitende-forderungen">Einleitung: Zustandsbeschreibung und abzuleitende Forderungen</h3>
<p>Aufgrund sinkender Kosten für die Papierherstellung kam es in der ersten Hälfte des 19. Jahrhunderts zu einem so explosionsartigen Anstieg des Buchdrucks, dass keine einzelne Bibliothek mehr den Ankauf aller relevanten Literatur bewältigen konnte. Daraufhin entstanden Staats- und Nationalbibliotheken, deren Aufgabe die Dokumentation der gesamten in einer Sprache oder einem Land gedruckten Veröffentlichungen war und ist. Dazu wurden sie mit einer <q>Ewigkeitsgarantie</q> versehen, das heißt einer langfristig gesicherten Unterhaltung aus Steuermitteln. Zugleich wurden zur Erschließung des gespeicherten Wissens normierte Katalogisierungssysteme entwickelt, die standardisiert als Vorbilder oder Vorgaben durch andere Bibliotheken genutzt werden konnten.</p>
<p>Hinsichtlich des heute ebenfalls exponentiell wachsenden digital gespeicherten Daten befinden wir uns meines Erachtens heute in einer vergleichbaren Situation. Nur ist aus der zweidimensionalen Gutenberg-Galaxis des bedruckten Papiers ein drei-, eigentlich – wegen der zeitlichen Komponente – sogar vierdimensionales digitales Universum mit Milliarden von Galaxien geworden. Selbst wenn für die Zukunft wie bisher nur ein Bruchteil dieser Informationen aufbewahrt werden kann und soll, so ist allein deren Umfang immer noch so riesig, dass er selbst nationale Institutionen überfordern dürfte.</p>
<p>Hinzu kommt aber noch ein grundlegendes Problem, vor dem die Bibliotheken bisher nicht standen: Die tatsächliche Verfügbarkeitsdauer dieser digitalen Daten sowie der zu ihrer Benutzung notwendigen Software und selbst der Hardware tendiert aus der Sicht des Historikers gegen Null: Die von Forschungsförderorganisationen für die nun zunehmend – von den Wissenschaftlern! – eingeforderten <q>Forschungsdatenmanagementpläne</q> zielen auf Zeiträume von 10-15, eventuell auf 20 und im Idealfall auf 50 Jahre ab… Aus dem Blickwinkel der Bibliothek, die jahrhundertealte Bücher und jahrtausendealte Papyri oder Tontafeln aufbewahrt, ist dies geradezu lächerlich…</p>
<p>Inzwischen gibt es erste Ansätze zur Entwicklung beispielsweise <em>Nationale Forschungsdateninfrastrukturen</em>. Dabei sollen durch Konsortien betroffener Institutionen Vorschläge für (eine) solche Infrastruktur(en) ausgearbeitet und in einem Wettbewerb ausgewählt werden, an dessen Ende dann ein System stehen soll, das geeignet sein müsste,</p>
<ul>
<li><p>die vorhandenen und zukünftigen Daten aus Forschungsprojekten und</p></li>
<li><p>die zumeist projektspezifisch angepasste Software (zum Beispiel Datenbankenschemata und Benutzerinterfaces) aufzunehmen sowie</p></li>
<li><p>gegebenfalls die dazugehörige Hardware zu simulieren.</p></li>
</ul>
<p>Diese Bemühungen kommen eigentlich circa 25 Jahre zu spät, denn nicht nur das seit Anfang der 1970er Jahre unter der Bezeichnung Arpanet bestehende heutige Internet wurde vor allem durch die Wissenschaft genutzt. Auch das 1991 von Tim Berners-Lee am CERN vorgestellte World Wide Web (WWW) und sein grundlegendes HyperText Transport Protocol (HTTP) waren <em>in erster Linie</em> für die schnelle Bereitstellung wissenschaftlicher Ergebnisse, ihren schnellstmöglichen Austausch und <em>ihre Verknüpfung</em> untereinander gedacht. Die Zahl der seitdem gestarteten, auf Digitalisierung basierenden Forschungsprojekte wuchs vermutlich exponentiell – ebenso wie die Zahl jener Projekte darunter, deren Förderung inzwischen eingestellt wurde und deren Daten und Ergebnisse längst wieder im <q>information black hole</q> (Cerf) verschwunden sind.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Vinton <q>Vint</q> Cerf – mit Robert E. <q>Bob</q> Kahn als Entwickler der Protokollfamilie TCP/IP zu Anfang der 1970er Jahre einer der <q>Gründerväter</q> des Internet – warnt seit einigen Jahren davor, dass wir nicht nur Forschungsdaten, sondern <em>alle</em> digitalen Daten verlieren werden:</p>
<blockquote>
<p><q>We are nonchalantly throwing all of our data into what could become an information black hole without realising it. We digitise things because we think we will preserve them, but what we don’t understand is that unless we take other steps, those digital versions may not be any better, and may even be worse, than the artefacts we digitised.</q><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</blockquote>
<p>Gemäß Cerf läuft unser digitales Zeitalter aus der Sicht zukünftiger Generationen Gefahr, ein <q>digital dark age</q> zu werden, weil der größte Teil unserer Kommunikation und Forschung, eigentlich die gesamte Kultur im weitesten Sinne, inzwischen in digitalisierter Form über das Internet gespeichert und ausgetauscht wird – einer Struktur, die zwar für diese massenhafte Anwendung nie ausgelegt war, sie bis heute aber erstaunlich gut bewältigt.</p>
<p>Allerdings ist die gesamte heutige Hard- und Software in absehbarer Zeit obsolet – und es ist fraglich, ob und wie zukünftige Generationen Interesse, Zeit, Geld und Arbeitskraft aufbringen werden, unsere dann <q>alten</q> – und aus ihrer Sicht vermutlich auch: veralteten – Daten zu retten.</p>
<p>Man sollte Cerfs Warnungen ernst nehmen, denn er ist jemand, der das Internet entscheidend mitprägte und seine Entwicklung in maßgeblicher Position begleitete. Sein eigener Vorschlag angesichts des drohenden Daten-Nirwanas ist die Schaffung einer Meta-Maschine, die er <em>Digital Vellum</em> nennt:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Sie soll im Wesentlichen aus Hardware samt Betriebssystem bestehen, welche eine <q>virtuelle Maschine</q>, also einen <q>simulierten Computer</q> bereitstellt, der die Daten <em>und</em> die zugehörige Software langfristig aufbewahren kann. Strukturell dürfte dies im Grunde der zur Zeit geplanten <em>Nationalen Forschungsdateninfrastruktur</em> entsprechen. Allerdings ist Cerfs <em>Digital Vellum</em> bereit einige Jahre in der Entwicklung und meines Wissens noch nicht fertig, geschweige denn: frei verfügbar. Sicherlich hätte ein Konzern wie Google, zu dessen Vizepräsidenten Cerf gehört, die Mittel, eine solche Entwicklung schnell voran zu treiben – aber ein profitorientiertes Unternehmen kann kaum daran interessiert sein, eine solche Lösung kostenlos (und werbefrei) als freie Software mit gegebenenfalls ebenso freiem Hardware-Design bereit zu stellen. Von Unternehmen, die ihr Geld gerade damit verdienen, ihren Kunden immer neue Versionen ihrer (Hard- und) Software zu verkaufen, kann dies erst recht nicht erwartet werden. Eine Abhängigkeit von solchen kommerziellen Interessen muss also – nicht zuletzt auch aufgrund der in der Regel vermutlich befristeten Existenzdauer solcher Firmen – ausgeschlossen werden: Ein in einer solchen virtuellen Maschine abgelegtes Word-Programm wird beim Start zum Öffnen eines 2020 erzeugten Dokuments im Jahre 2050 spätestens schon keinen Kontakt mehr zum Authentifizierungsserver herstellen können und also Start und Datenzugriff verweigern.</p>
<p>Man mag einwenden, dass ja der Inhalt des Dokuments in einem XML-Dialekt vorliegt und so prinzipiell rekonstruiert werden kann: Aber spätestens, wenn spezifische Layouts, Bilder oder Funktionen ebenfalls erhalten werden sollen, wird dies wohl kaum möglich sein. Zudem lassen sich in Textdokumenten zwar Forschungsergebnisse zusammengefasst darstellen, Forschungsdaten selbst aber schon kaum noch erfassen – zumindest nicht in einer sinnvollen digitalen Weise, die über eine Schreibmaschine hinausgeht.</p>
<p>Stattdessen wird es kaum noch ein Forschungsprojekt geben, das nicht eine mehr oder weniger aktuelle Datenbank-Management-Software nutzt. Zusätzlich wird zumeist eine spezifische Datenbank-Struktur (relational, objekt-orientiert, objekt-relational, graphenbasiert…) gemäß dem Forschungsdesign des Projekts entwickelt. Die Daten werden meist mit zusätzlicher Software beispielsweise für die Benutzeroberflächen erfasst und genutzt, die in sich ebenfalls rasant entwickelnden Skriptsprachen verfasst ist.</p>
<p>Versucht man sich zu vergegenwärtigen, <em>wie viele</em> Komponenten in der Hard-, vor allem aber in der Software am Zustandekommen einer jeweils projektspezifischen IT-Struktur beteiligt sind, erscheint es nahezu ausgeschlossen, diese vollständig und dauerhaft für sehr viele laufende und abgeschlossene Projekte konservieren zu können: Schon der Versuch, innerhalb eines einzelnen Projekts, erst recht vielleicht einige Jahre nach dessen Abschluss, eine Migration auf eine neuere Software-Version durchzuführen, ist in der Regel zumindest sehr kompliziert, häufig aber undurchführbar, beispielsweise weil keine ausreichende Dokumentation vorliegt und die ursprünglichen Entwickler und Wissenschaftler – oft in Personalunion –, deren Überlegungen und Forschungsinteressen das Design des gesamten Projekts prägten, gar nicht mehr verfügbar sind. Und selbst, <em>wenn</em> sie es heute sind und/oder eine tatsächlich vollständige Dokumentation hinterlassen haben, kann man kaum ernsthaft erwarten, dass beispielsweise in 100 Jahren oder später noch jemand Zeit, Geld und Arbeitskraft aufzuwenden bereit oder in der Lage sein wird, ihre Daten in einer dann nutzbaren Form – mitsamt der Software, der virtuellen Maschine und dem <em>digital vellum</em> oder der dazugehörigen Forschungsdateninfrastruktur – <q>wiederzubeleben</q>. Die unter anderem deshalb angestrebte, aber meines Wissens selten vollständig realisierbare <q>Abstraktion</q> der Daten und ihrer Strukturierung von einer spezifischen Hard- und Software – beispielsweise in XML – soll dies zwar verhindern, aber zumindest in den <em>Digital Humanities</em> ist mir keine Projekte bekannt, denen es zum Beispiel gelungen wäre, Daten mittels des ISO-Standards <em>CIDOC-CRM</em> auszutauschen. Aber dies bestätigt nur meine Erfahrungen aus circa 35 Jahren im Umgang mit Computern und circa 25 Jahren Mitwirkung in Projekten der <em>Digital Humanities</em>.</p>
<p>Versteht man unter Langfristigkeit und Nachhaltigkeit <em>nicht</em> – wie gegenwärtig wohl noch diverse nationale Forschungsförderungsinstitutionen – eine Zeitspanne von 15-20, bestenfalls 50 Jahren (nur für TXT und PDF/A), hält man also digitale Forschungsdaten und -ergebnisse für wichtig genug, um sie genau so lange verfügbar zu halten, wie solche früherer Jahrhunderte auf Papier, wird man erst recht Zweifel an heutigen Lösungsansätzen hegen: Denn die tatsächlich <em>langfristige</em> Speicherung von Daten und Software verlangt nicht nur ihre einmalige Übertragung in ein <em>digital-vellum-</em>artiges System, sondern die <em>fortwährende</em> Migration auf die nächste oder spätestens übernächste Generation von Hard- und Software, und zwar nicht nur für die Projektdaten, sondern auch für das <q>Hostsystem</q>, beziehungsweise eben das <em>Digital Vellum</em> selbst.</p>
<p>Dies erfordert demnach, wie Cerfs Kollege Robert E. Kahn, verdeutlichte, auch eine <em>soziale Infrastruktur</em>, eine Institution, die von <em>langfristiger</em> Dauer und in der Lage ist, solche Systeme samt der erfassten Daten zu betreuen.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Dem möchte ein Vorschlag Alan Kays für <q>digitale Tontafeln</q> (<em>cuneiform tablets</em>) begegnen: Kay, der seit Ende der 1960er Jahre beim XEROX Palo Alto Research Center (PARC) die Entwicklung der Programmiersprache und -entwicklungsumgebung <em>Smalltalk</em> samt <q>virtueller Maschine</q>, die Konzeption und Realisierung graphischer Benutzeroberflächen sowie des objekt-orientierten Paradigmas entscheidend prägte, schlägt <q>selbsterklärende</q> Datenträger – aktuell noch in der Form von CDs oder DVDs – vor, die es selbst dem interessierten <q>Archäologen</q> noch in tausenden von Jahren erlauben sollen, in kürzester Zeit und ohne allzu großen Aufwand alle erforderlichen Strukturen zu rekonstruieren, die notwendig sind, um den Datenträger zu lesen und die darauf <q>eingefrorene</q> virtuelle Maschine – nicht zufällig am Beispiel eines <em>Smalltalk</em>-Systems demonstriert – samt der darin erfassten Daten zu nutzen. Für die langfristige Speicherung nicht mehr zu verändernder Daten mag dieses Konzept geeignet erscheinen: Aber es geht damit eigentlich nur wenig über die <q>Speicherung</q> auf bisherigen Schriftträgern wie Stein, Ton, Metall, Pergament oder Papier hinaus. Ein <em>wesentlicher</em> Vorteil der Digitalisierung ist aber gerade, dass große Mengen erfasster Daten und zu ihrer Nutzung und Auswertung geschriebener Software nicht nur <q>fix(iert)</q> genutzt, sondern auch kontinuierlich erweitert und angepasst werden können – möglichst ohne, dass die Informationen oder Metadaten der ursprünglichen Daten verloren gehen oder verfälscht werden können.</p>
<p>Zusammenfassend lässt sich sagen: Es bestehen folgende Probleme:</p>
<ul>
<li><p>aktuelle Nichtverfügbarkeit langfristig (= &gt; 50 Jahre) stabiler Systeme;</p></li>
<li><p>damit nicht garantierten Nutzbarkeit heutiger digitaler Daten;</p></li>
<li><p><q>Wildwuchs</q> durch kommerzielle Interessen <q>geschützter</q> Hard- und Software;</p></li>
<li><p><em>zeitlich</em> begrenzte Finanzierung digitaler Forschungsprojekte;</p></li>
<li><p>damit ausgeschlossenen Möglichkeit, die <em>notwendige</em> regelmäßige Migration aller Daten und Software <em>langfristig</em> zu sichern;</p></li>
<li><p>und die vernachlässigten Gefahr durch grundlegende IT-Paradigmenwechsel.</p></li>
</ul>
<p>Angesichts all dessen erscheint es mir geradezu <em>ausgeschlossen</em>, dass der Versuch tatsächlich gelingen kann, diesem <q>Dschungel</q> oder <q>Zoo</q> an Hard- und idiosynkratischer Software in irgendeiner Form Dauerhaftigkeit zu verleihen, indem man das alles in ein weiteres, nicht minder komplexes Forschungsdatenmanagementsystem, ein Hostsystem wie das <em>Digital Vellum</em>, überträgt , zumal dessen langfristige Existenz von derjenigen einzelner Institutionen oder -verbünde und vor allem einer verlässlichen Finanzierung abhängig wäre, die zumindest heutige Forschungsförderinstitutionen gar nicht leisten können und wollen.</p>
<p>Hinsichtlich der erwähnten, von Robert E. Kahn geforderten <q>social structure</q>, welche für die langfristige Datensicherheit unverzichtbar ist, darf zudem gelten, dass heutige Firmen und Institute diesbezüglich wohl nicht ausreichend sind; alte Universitäten, Archive oder Museen oder eben Bibliotheken kämen da vielleicht eher in Frage, zumal gerade Bibliotheken und Archive zudem nicht nur über die Kompetenz zur Speicherung von Daten, sondern auch zu deren Erschließung verfügen.</p>
<h3 id="lösungsvorschlag-ein-radikaler-neubeginn">Lösungsvorschlag: ein radikaler Neubeginn</h3>
<p>Aufgrund des bisher Geschilderten ist es meines Erachtens notwendig, eine vorzugsweise internationale Institution mit <q>Ewigkeitsgarantie</q> zu schaffen, die vergleichbar einer Nationalbibliothek, einem Nationalarchiv oder -museum die dauerhafte Speicherung und Verfügbarkeit <em>aller</em> ihr übertragenen Daten garantieren kann. Dabei kann es sich jedoch nicht um eine einfache Sammlung gängiger, nahezu beliebig komplexer, projektspezifischer Hard- und Softwarelösungen handeln. Stattdessen sollte diese Institution der Entwicklung und Pflege (<em>Maintainance</em>) einer <em>vollständig neuen</em> <em>Hard- und Software-Infrastruktur</em> dienen, welche alle Erfahrungen der bisherigen Entwicklungen berücksichtigt, vor allem bekannte Fehler vermeidet, möglichst sicher und nachhaltig (<q>sustainable</q>) konzipiert ist und nach den Regeln der <em>Free Software</em> nicht nur im Quellcode zugänglich (= open source), sondern auch für alle Interessierten und Zwecke frei nutzbar sein sollte. Allerdings müsste diese Institution <em>immer</em> das Recht haben, die Rück- beziehungsweise Übernahme von Projektdaten und -software zu verweigern, wenn deren Anpassungen nicht zuvor mit ihr abgesprochen, auf Kompatibilität mit dem <q>Hauptsystem</q> geprüft und dokumentiert worden sein sollten.</p>
<p>Das System selbst müsste zum einen auf einer freien, offenen, modularen Hardware-Architektur (oder einer Gruppe von Architekturentwürfen) beruhen, die von jeder interessierten Firma oder Institution implementiert werden kann: So könnten nicht nur kostspielige Monopolbildungen – Stichwort: <em>Wintel-</em>Allianz – vermieden, sondern auch notwendige Änderungen schneller umgesetzt werden. Dieses System müsste zudem den schnellen Austausch aller Komponenten ohne Gefährdung der Gesamtarchitektur erlauben. Mit dieser Hardware wäre die eigentliche – natürlich ebenfalls freie – Software (Betriebssystem, <em>Middleware</em> und Anwendungssoftware wie Office- und Datenbank-Programme aber auch Peripherie-Treiber, Programmiersprachen et cetera) nur durch eine möglichst kleine <q>virtuelle Maschine</q> verbunden, die sich schnell und einfach an sehr viele oder neue Hardware-Strukturen anpassen ließe: <em>Smalltalk</em> läuft beispielsweise in einer solchen, sogar zur Laufzeit veränderbaren <q>virtuellen Maschine</q> auf über 100 Prozessorarchitekturen.</p>
<p>Die für die eigentliche Projektarbeit notwendigen Software-Pakete sollten dann zum Beispiel nur je <em>eine</em>, jedoch umfassende Lösung für Office-Funktionen und Berechnungen sowie Datenbanken bereitstellen und gegebenenfalls neu zu definierende Standard-Datenformate verwenden. Die Datenbanksoftware müsste jeweils eine Lösung für die verschiedenen gebräuchlichen Paradigmen – relational, objekt-orientiert, graphenbasiert et cetera – enthalten. Das Ganze wäre ähnlich den heutigen <em>Docker Containern</em> in Linux zwischen verschiedenen Systemen weitgehend abstrahiert übertragbar.</p>
<p>Idealerweise sollten <em>alle</em> in so einem System vorhandenen Daten über eindeutige Identifier in der Art von <q>URLs</q> adressierbar sein, wie es im Prinzip im System HTTP/WWW mit der an die Unix-Struktur angelehnten Trennung hierarchischen Struktur bereits angelegt ist. Damit wären die darin abgelegten Daten jederzeit und von überall erreichbar. Links zwischen diesen Daten wären ab einem gemeinsamen Wurzelverzeichnis stabil. Diese sollten aber endlich – wie in den ebenfalls schon fast 25 Jahre alten WikiWiki-Systemen automatisch ihre Backlinks erzeugen: Das heißt, beim Verlinken eines Datenobjekts X mittels eines Link-Objekts Y auf ein anderes Datenobjekt Z (X =&gt; Y =&gt; Z) würde in Z zugleich der Back-Link (Z =&gt; Y&#8217; =&gt; X) angelegt, so dass Kommentare, Zitationen und so weiter immer in beide Richtungen sichtbar wären: Die heute übliche unidirektionale Verlinkung bedeutet ja gegenüber der Fußnote auf dem Papier keinen Fortschritt, solange nicht im verlinkten Datensatz <em>von Hand</em> ein Back-Link angelegt wird.</p>
<p>Natürlich müssten alle Objekte beziehungsweise ganze Strukturbäume innerhalb des Gesamtsystems oder seiner in virtuellen Maschinen laufenden Untersysteme immer schützbar und dem Urheber eindeutig zuweisbar sein. Bei entsprechendem Design ließen sich so auch <q>Kataloge</q> sämtlicher Daten jederzeit aktuell erstellen, weil das gesamte System im Prinzip zugleich seine eigene Datenbank wäre. Auch dies eine Idee, die natürlich nicht neu ist.</p>
<p>Damit könnte dann beispielsweise auch endlich das Problem eine echten <em>Open Access</em> Publishing sowie der Open Data für Forschungsdaten dauerhaft gelöst werden: Was nicht innerhalb des Systems auffindbar, erreichbar, verlinkbar <em>und frei nutzbar</em> wäre, würde mit der Zeit an Bedeutung verlieren. Durch die vollständige Offenheit der gesamten Plattform könnten zugleich Bibliotheken und Universitäten oder andere Forschungseinrichtungen jeweils eigene, zueinander kompatible und über Netzwerke verbundene Repositorien anlegen. Und selbst die <em>Peer review</em> könnte ihrem hochtrabenden Namen endlich gerecht werden, indem <em>alle</em> Mitwirkenden untereinander tatsächlich als <em>Peers</em>, also Ebenbürtige, aufträten und <em>jede/r</em> jederzeit die Veröffentlichungen und Daten der anderen sehen, nutzen und kommentieren könnte, wie es Tim Berners-Lee für das WWW ursprünglich vorsah. Eine spezielle <q>Subplattform</q> für <q>Verlage</q> enthält das WWW dagegen nicht – sie hätte den Erfolg seines Projekts wohl auch aus kommerziellen Interessen verhindert.</p>
<p>Die immer wieder ins Feld geführte, angeblich nur durch Verlage zu garantierenden Qualitätskontrolle ist meines Erachtens ein Schein-Argument, denn während sie bisher angeblich durch Kleinstgruppen von Verlegern, Lektoren, Redakteuren und Gutachtern mit je eigenen – gerade im anonymen Review nicht immer offensichtlichen – Interessen gewährleistet sein soll, würde sie in diesem offenen System von Repositorien durch die prinzipiell <em>alle</em> Fachwissenschaftler umfassende <em>scientific community</em> tatsächlich realisiert. Die meines Erachtens zweifelhaften heutigen Bibliometrie-Systeme würden dadurch ebenso obsolet wie <q>Zitierseilschaften</q> leicht erkennbar.</p>
<p>Die zu schaffende Institution sollte also nicht nur die strukturelle Form einer Bibliothek haben, sondern meiner Meinung nach auch an deren Arbeitsweise orientiert sein: Das heißt, sie würde das IT-System (wie bisherige Bibliotheken ihre Katalogsysteme) entwickeln und betreiben und an Kooperationspartner in Form einer virtuellen Maschine für deren eigene Projekte <q>ausliefern</q>. Die zu fordernde Kontrolle des Entwicklungsprozesse erlaubte die Garantie der (Rück-)Übernahme aller Daten nach Projektabschluss – wenn diese nicht ohnehin von vornherein auf einer von der Institution bereitgestellter <em>Cloud</em> <q>Software as a Service</q> nutzen sollte. Regeln für den Zugriff auf beispielsweise sensible Daten oder die Erweiterung der Daten beziehungsweise ihre Übernahme in zukünftige Projekte ließen sich in solch einer einheitlichen Systemplattform nicht nur relativ leicht und standardisiert vereinbaren, sondern dank der modularen Struktur von gegebenfalls ineinander <q>verschachtelbaren</q> oder miteinander verlinkbaren virtuellen Maschinen auch leicht(er) realisieren.</p>
<p><em>Last but not least</em> wäre eine solche institutionalisierte Infrastruktur nicht nur datentechnisch <em>nachhaltiger</em> als alles bisher Verfüg- und wohl auch Vorstellbare. Dank des offenen Designs von Hard- und Software ließen sich zweifellos auch <q>modulare</q> Optimierungen schnell realisieren, die eine energetischen/ökologischen <em>und</em> technischen Nachhaltigkeit (beispielsweise durch Recycling von Bauelementen und geringstmöglichen Stromverbrauch) ermöglichten.</p>
<p>Man mag einwenden, dass ein solcher kompletter Neustart einer gesamten Forschungs-/IT-Infrastruktur <em>viel zu teuer</em> sei. Tatsächlich dürfte sie sich im dreistelligen Million- oder ein-, vielleicht sogar zweistelligen Milliardenbereich bewegen – insbesondere, wenn man beispielsweise die besten und entsprechend vergüteten Spezialisten engagieren will… Dies ist bisher wohl nur Google, Microsoft, Facebook, Apple oder derNSA möglich. Zumindest im letzteren Falle (beziehungsweise dem der Geheimdienste generell) könnte man sich ja aber auch einmal fragen, ob nicht die langfristige Rettung aller wissenschaftlichen Ergebnisse eine höhere gesellschaftliche Priorität haben sollte, als die Überwachung aller Bürger. Ganz abgesehen davon, dass auch die riesigen Investitionen der genannten und ähnlicher Firmen letztlich durch deren Kunden finanziert werden, die zugleich in der Regel auch Steuerzahler sind: Das Geld ist also prinzipiell längst vorhanden, nur falsch alloziert.</p>
<p>Ein weiterer Einwand könnte neben den Kosten für die Erstellung eines solchen Systems in den Kosten für die Migration aller heute vorhandenen Projektdaten von herkömmlichen Systemen auf dieses neue System gesehen werden. Aber solche Migrationsschritte werden in Zukunft <em>immer wieder</em> nötig und in absehbarer Zeit nicht mehr <em>möglich</em> sein, weil die verwendete Software nicht mehr weiterentwickelt oder neuesten Hardware- und Betriebssystemen nicht mehr unter vertretbarem Aufwand angepasst werden kann. Ich schätze, dass die <em>einmalige</em> Migration vorhandener, bewahrenswerter Daten und ihrer projektspezifischen Software auf das neue System <em>immer</em> weniger kosten wird, als deren Migration über die nächsten zwei oder drei Generationen herkömmlicher Systeme. Zwar wären natürlich auch innerhalb dieser neuen Gesamtplattform solche Migrationen notwendig, ihr Aufwand ließe sich aber auf ein Minimum reduzieren, weil <em>genau dies</em> bereits bei der Konzeption eine grundlegende Anforderung. Nach meinen Erfahrungen hat zumindest in den bisherigen institutionellen Strukturen <em>niemand</em> die Mittel und Möglichkeiten, oft nicht einmal ein Interesse, eine solche herkömmliche Migration auch nur <em>einmal</em> vorzunehmen – was dann zum <q>Friedhof der Projektleichen</q> jeweils ein weiteres <q>Datengrab</q> hinzufügt.</p>
<p>Man könnte zum Vergleich mit dieser scheinbar gigantischen Zukunftsinvestition auch andere kostspielige öffentliche Investitionen heranziehen, deren Nutzen für Staaten und ihre Bürger geringer, zweifelhaft oder sogar <q>negativ</q> – also ein Schaden – ist. Stichworte hierzu wären Kriege und Rüstung, industrielle Landwirtschaft, Banken-, also Spekulantenrettung, Steuerbetrug – allein <em>Cum-Ex</em> dürfte ein Vielfaches der einzuplanenden Projektmittel gekostet haben –, aktuelle Drohnen- und Mautdebakel und andere, absehbare Steuergeldverschwendung wie in allen bekannten ÖPP-Projekten.</p>
<p>Man sollte auch nicht vergessen, dass im Umfeld eines solchen offenen Systems eine Vielzahl von Dienstleistungen – von der Hardware-Entwicklung und Produktion bis zur Anpassung der Standard-Software an spezifsiche Projektbedürfnisse – entstehen und neben qualifizierten und langfristig sicheren Arbeitsplätzen auch wiederum Steuereinnahmen generieren würde.</p>
<h3 id="literatur">Literatur</h3>
<p>[Alle Internetquellen zuletzt abgerufen am 31.10.2019]</p>
<p>– Cerf 2014 = Cerf, Vinton: A Long Term View of the World Wide Web [Präsentation]: <a href="https://vimeo.com/110794988" class="uri">https://vimeo.com/110794988</a></p>
<p>– Cerf 2015a = Cerf, Vinton; Sample, Ian: [Interview] Google Boss warns of ‚forgotten century’ with email and photos at risk, in: <em>The Guardian</em>, 13. Februar 2015: <a href="https://www.theguardian.com/technology/2015/feb/13/google-boss-warns-forgotten-century-email-photos-vint-cerf" class="uri">https://www.theguardian.com/technology/2015/feb/13/google-boss-warns-forgotten-century-email-photos-vint-cerf</a></p>
<p>– Cerf 2015b = Cerf, Vinton: Digital Vellum. [Präsentation]: <a href="https://www.youtube.com/watch?v=STeLOogWqWk" class="uri">https://www.youtube.com/watch?v=STeLOogWqWk</a></p>
<p>– Kahn 2016 = Kahn, Robert E.: Challenges and Opportunities for Digital Preservation (Keynote auf der Konferenz IPRES 2016 in Bern), Astract unter: <a href="https://ead.nb.admin.ch/web/ipres2016/frontend/indexda54.html?page_id=1166" class="uri">https://ead.nb.admin.ch/web/ipres2016/frontend/indexda54.html?page_id=1166</a></p>
<p>– Kay / Nguyen 2015 = Nguyen, Long Tien; Kay, Alan: The Cuneiform Tablets of 2015. Viewpoints Research Institute, VPRI Technical Report TR-2015-004, Los Angeles: 2015: <a href="http://www.vpri.org/pdf/tr2015004_cuneiform.pdf" class="uri">http://www.vpri.org/pdf/tr2015004_cuneiform.pdf</a></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Beispielsweise Cerf 2015; zum WWW siehe Cerf 2014.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Cerf 2015<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Cerf 2015b.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Kahn 2016.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
<hr />
<p><strong>Dr. phil. Bernd Kulawik</strong>, Bern/Berlin. Kontakt: be_kul@me.com</p>
</div>

    

</article>




        <footer class="footer">
<hr>
<p><a href="https://creativecommons.org/licenses/by/4.0/">All content licensed under Creative Commons Attribution 4.0 Unported (CC BY 4.0)</a>, if not otherwise stated.
</p>
<p>LIBREAS. Library Ideas wird herausgegeben am <a href="https://www.ibi.hu-berlin.de/">Institut für Bibliotheks- und Informationswissenschaft</a> der <a href="https://www.hu-berlin.de/">Humboldt-Universität zu Berlin</a></p>
<p>Hosted on <a href="http://github.com/libreas/libreas.github.io">GitHub</a>, made with <a href="http://octopress.org/">Jekyll/Octopress</a> and <a href="http://johnmacfarlane.net/pandoc/">pandoc</a></p>

<p>ISSN: 1860-7950</p>


    LIBREAS. Library Ideas

(last updated: <a href="https://github.com/libreas/libreas.github.io/commits/master">2020-12-28</a>)</p>

</footer>
      </div>
    </div>
<label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
